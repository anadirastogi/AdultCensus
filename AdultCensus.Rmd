---
title: "Adult Census Project"
author: "Group 4 (Adi, Pam, Jennifer, Jason)"
date: "April 19, 2019"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdownuuuy

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

##Data preprocessing
##Clear workspace
```{r}
rm(list = ls())
```

##Libraries and packages
```{r}
install.packages("e1071")
library(e1071)
library(tidyverse)
library(ggplot2)
```

##Read file and view data

We read the file into R and look at the data to get a sense of what we are working with.
```{r}
mydata<-read.csv(file = "adult.csv")
head(mydata)
```

##Save dataset in tidy way 
```{r}
mydata<-as_tibble(mydata)
View(mydata)
```

##Check for null values and count the number of null values

Since there are a number of records in our data that contain "?" we first need to let R know that these are missing values
```{r}
## null values appear as a questionmark in the data
## set ? = NA
mydata$workclass[mydata$workclass == "?"] <- NA
mydata$occupation[mydata$occupation == "?"] <- NA
mydata$native.country[mydata$native.country == "?"] <- NA

## Count number of missing values
sum(is.na(mydata))

## we have 4262 missing values in our dataset
```

## analyze missing values
```{r}
## want to look at missing values to better understand the source/cause of missing data
## quick visual inspection shows us that wherever occupation is NA, workclass is also NA
mydata%>%
  filter(is.na(occupation))%>%
  arrange(education, marital.status)

## upon a quick visual inspection, there does not seem to be an obvious relationship between the records with missing information, therefore we will seek to simply remove the missing values
```

## Remove missing values
```{r}
mydata2<-na.omit(mydata)
```

## Data Visualization

# a bar chart showing relationship between income and education
```{r}
mydata2%>%
ggplot(aes(x=education.num, y=1, fill=income))+ 
  geom_bar(stat = "identity", position = "stack")

```
## plotting relationships of the other attribues with income (target variable)
```{r}

## Numeric Variables

##ecucation.num vs income
mydata2%>%
  ggplot(aes(x=education.num,fill=income))+geom_histogram(binwidth=1,position="stack")+
  facet_wrap(~income)

##age vs income
mydata2%>%
  ggplot(aes(x=age,fill=income))+geom_histogram(binwidth=15)+
  facet_wrap(~income)

mydata2%>%
  ggplot(aes(x=age,fill=income))+geom_histogram(binwidth=1, colour="black", position="stack")

##capital.loss vs income
mydata2%>%
  ggplot(aes(x=capital.loss,fill=income))+geom_histogram(binwidth=500)+
  facet_wrap(~income)

##capital.gain vs income
mydata2%>%
  ggplot(aes(x=capital.gain,fill=income))+geom_histogram(binwidth=20000)+
  facet_wrap(~income)

## hours worked vs income
mydata2%>%
  ggplot(aes(x=hours.per.week,fill=income))+geom_histogram(binwidth=5, colour="black", position="stack")

```
```{r}
## Categorical variables

## occupation vs income
mydata2%>%
  ggplot(aes(x=occupation,fill=income))+geom_bar(size=5)+coord_flip()

## education vs income
mydata2%>%
  ggplot(aes(x=education,fill=income))+geom_bar(size=5)+coord_flip()
```
#
```{r}
set.seed(1)   # set a random seed 
index <- sample(30162,6000) # random selection of indices. 
test <-mydata2[index,]       # save 20% as a test dataset
training <- mydata2[-index,]   # save the rest as a training set
```

#Tree-based classification model
```{r}
library(rpart)
library(rpart.plot)
training_model<-rpart(income~.,           # model formula
                data=training,                     # dataset
                method="class",                   # "class" indicates a classification tree model 
                control=rpart.control(cp=0.03))   # tree control parameters. 
rpart.plot(training_model)   # tree plot
```
# predicting probabilities/class labels for test data
Now, the model will be evaluated on a test data. For this, we  apply the model to the test dataset and get the predicted values. It can be done by providing a dataset name with a model to `predict()` function. 
```{r}
test$ct_pred_prob<-predict(training_model,test)[,2]
test$ct_pred_class<-predict(training_model,test,type="class")
test
```
# k-fold Cross-validation 
```{r}
set.seed(1)   # set a random seed 
full_tree<-rpart(income~.,
                     data=training, 
                     method="class",
                     control=rpart.control(cp=0))

rpart.plot(full_tree)
```


```{r}
printcp(full_tree)   # xerror, xstd - cross validation results  
```

```{r}
plotcp(full_tree)    
```


```{r}
min_xerror<-full_tree$cptable[which.min(full_tree$cptable[,"xerror"]),]
min_xerror

# prune tree with minimum cp value
min_xerror_tree<-prune(full_tree, cp=min_xerror[1])
rpart.plot(min_xerror_tree)
```

Let's consider mim_xerror_tree as the best pruned tree, and get the prediction. 
```{r}
bp_tree<-min_xerror_tree
test$ct_bp_pred_prob<-predict(bp_tree,test)[,2]
test$ct_bp_pred_class=predict(bp_tree,test,type="class")

table(test$ct_bp_pred_class==test$income)  # error rate
table(test$ct_bp_pred_class,test$income, dnn=c("predicted","actual"))  # confusion table on test data
```
accuracy = 5137 / (5137 + 863) = 0.8562

```{r}
ad_model<-rpart(income~.,           # model formula
                data=training,                     # dataset
                method="class",                  
                control=rpart.control(cp=0.004))
rpart.plot(ad_model)   # tree plot
test$ct_ad_pred_prob<-predict(ad_model,test)[,2]
test$ct_ad_pred_class<-predict(ad_model,test,type="class")
table(test$ct_ad_pred_class==test$income)  # error rate
table(test$ct_ad_pred_class,test$income, dnn=c("predicted","actual"))  # confusion table on test data
```

Conclusion: As adding more complexity does not improve the performance much, we will choose cp as 0.004. The tree result will be manageable to understand, and the performance seems to be great.

<<<<<<< HEAD
Check the accuracy of the model by comparing the the actual values (income) and the predicted values (ct_pred_class).
```{r}
table(test$income==test$ct_pred_class)    
```
accuracy = 5037 / (5037 + 963) = 0.8395
=======
## Looking at target variable
```{r}
## check how many records have income >50k and how many do not
table(mydata2$income)
## we see that most of the records indicate income of <=50k, only ~25% have >50k

mydata2%>%
  ggplot(aes(x=income, fill=education))+geom_bar(size=5)

mydata2%>%
  ggplot(aes(x=income, fill=sex))+geom_bar(size=5)
## men are a bigger % of the high earners than females

```
###### Split data into training and testing set
```{r}
set.seed(1)
idx <- sample(2,nrow(mydata2),replace=TRUE,prob=c(.8,.2)) # "idx" is an index, which tells us which observations were assigned to the training (idx=1) and validation (idx=2) samples 
table(idx) # confirm 80/20 split
```
###### Modeling
### SVM
```{r}

model_svm<-svm(formula=income~age+education.num+occupation, # model formula 
               data=mydata2[idx==1,],                   # dataset
               kernel="linear", # linear decision boundary
               cost=1,        # there are paremeters that are used to tune the model 
               scale=FALSE)

model_svm
```
```{r}
#plot(model_svm, mydata2,age~education.num~occupation)
```


## Tuning
```{r}

```


## Validating
```{r}

```
>>>>>>> 95ced096e9933f96c25b04afbb7acdaf78b01b68

