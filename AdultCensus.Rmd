---
title: "Adult Census Project"
author: "Group 4 (Adi, Pam, Jennifer, Jason)"
date: "April 19, 2019"
output:
  html_document: default
  word_document: default
  df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

##Data preprocessing
##Clear workspace
```{r}
rm(list = ls())
```

##Libraries and packages
```{r}
#install.packages("caret")
library(caret)
```


```{r}
#install.packages("e1071")
library(e1071)
library(tidyverse)
library(ggplot2)
#install.packages("pROC")
library(pROC)
```

##Read file and view data

We read the file into R and look at the data to get a sense of what we are working with.
```{r}
mydata<-read.csv(file = "adult.csv")
head(mydata)
```

##Save dataset in tidy way 
```{r}
mydata<-as_tibble(mydata)
View(mydata)
```

##Check for null values and count the number of null values

Since there are a number of records in our data that contain "?" we first need to let R know that these are missing values
```{r}
## null values appear as a questionmark in the data
## set ? = NA
mydata$workclass[mydata$workclass == "?"] <- NA
mydata$occupation[mydata$occupation == "?"] <- NA
mydata$native.country[mydata$native.country == "?"] <- NA

## Count number of missing values
sum(is.na(mydata))

## we have 4262 missing values in our dataset
```

## analyze missing values
```{r}
## want to look at missing values to better understand the source/cause of missing data
## quick visual inspection shows us that wherever occupation is NA, workclass is also NA
mydata%>%
  filter(is.na(occupation))%>%
  arrange(education, marital.status)

## upon a quick visual inspection, there does not seem to be an obvious relationship between the records with missing information, therefore we will seek to simply remove the missing values
```

## Remove missing values
```{r}
mydata2<-na.omit(mydata)

head(mydata2)
```

## Data Visualization

# a bar chart showing relationship between income and education
```{r}
mydata2%>%
ggplot(aes(x=education.num, y=1, fill=income))+ 
  geom_bar(stat = "identity", position = "stack")

```
## plotting relationships of the other attribues with income (target variable)
```{r}

## Numeric Variables

##ecucation.num vs income
mydata2%>%
  ggplot(aes(x=education.num,fill=income))+geom_histogram(binwidth=1,position="stack")
  facet_wrap(~income)
##age vs income

mydata2%>%
  ggplot(aes(x=age,fill=income))+geom_histogram(binwidth=1, colour="black", position="stack")

##capital.loss vs income
mydata2%>%
  ggplot(aes(x=capital.loss,fill=income))+geom_histogram(binwidth=500)+
  facet_wrap(~income)

##capital.gain vs income
mydata2%>%
  ggplot(aes(x=capital.gain,fill=income))+geom_histogram(binwidth=20000)+
  facet_wrap(~income)

## hours worked vs income
mydata2%>%
  ggplot(aes(x=hours.per.week,fill=income))+geom_histogram(binwidth=5, colour="black", position="stack")

```
```{r}
##Marriage vs age
mydata2%>%
  ggplot(aes(x=age,fill=marital.status))+geom_histogram(binwidth=1,  position="stack")
```


```{r}
## Categorical variables

## occupation vs income
mydata2%>%
  ggplot(aes(x=occupation,fill=income))+geom_bar(size=5)+coord_flip()

## education vs income
mydata2%>%
  ggplot(aes(x=education,fill=income))+geom_bar(size=5)+coord_flip()

## native country
mydata2%>%
  ggplot(aes(x=native.country,fill=income))+geom_bar(size=5)+coord_flip()
##consider removing all other countries?
```

## Looking at target variable
```{r}
## check how many records have income >50k and how many do not
table(mydata2$income)
## we see that most of the records indicate income of <=50k, only ~25% have >50k

mydata2%>%
  ggplot(aes(x=income, fill=education))+geom_bar(size=5)

mydata2%>%
  ggplot(aes(x=income, fill=sex))+geom_bar(size=5)
## men are a bigger % of the high earners than females

```
###### Split data into training and testing set

```{r}
set.seed(1)
index <- sample(30162, 6000) # random selection of indices.
test <- mydata2[index,]       # save 20% as a test dataset
training <-mydata2[-index,]   # save the rest as a training set
```
#Tree-based classification model
```{r}
library(rpart)
library(rpart.plot)
training_model<-rpart(income~age+workclass+fnlwgt+education+education.num+marital.status+occupation+relationship+race+sex+capital.gain+capital.loss+hours.per.week+native.country,           # model formula
                data=training,                     # dataset
                method="class",                   # "class" indicates a classification tree model 
                control=rpart.control(cp=0.03))   # tree control parameters. 
rpart.plot(training_model)   # tree plot
```
# predicting probabilities/class labels for test data
Now, the model will be evaluated on a test data. For this, we  apply the model to the test dataset and get the predicted values. It can be done by providing a dataset name with a model to `predict()` function. 
```{r}
test$ct_pred_prob<-predict(training_model,test)[,2]
test$ct_pred_class<-predict(training_model,test,type="class")
test
```

Check the accuracy of the model by comparing the the actual values (income) and the predicted values (ct_pred_class).
```{r}
table(test$income==test$ct_pred_class)    
```
accuracy = 5037 / (5037 + 963) = 0.8395

# k-fold Cross-validation 
```{r}
set.seed(1)   # set a random seed 
full_tree<-rpart(income~age+workclass+fnlwgt+education+education.num+marital.status+occupation+relationship+race+sex+capital.gain+capital.loss+hours.per.week+native.country,
                     data=training, 
                     method="class",
                     control=rpart.control(cp=0))
rpart.plot(full_tree)
```


```{r}
printcp(full_tree)   # xerror, xstd - cross validation results  
```

```{r}
plotcp(full_tree)    
```


```{r}
min_xerror<-full_tree$cptable[which.min(full_tree$cptable[,"xerror"]),]
min_xerror
# prune tree with minimum cp value
min_xerror_tree<-prune(full_tree, cp=min_xerror[1])
rpart.plot(min_xerror_tree)
```

Let's consider mim_xerror_tree as the best pruned tree, and get the prediction. 
```{r}
bp_tree<-min_xerror_tree

training$ct_bp_pred_prob<-predict(bp_tree,training)[,2]
training$ct_bp_pred_class=predict(bp_tree,training,type="class")
table(training$ct_bp_pred_class==training$income)  # error rate
table(training$ct_bp_pred_class,training$income, dnn=c("predicted","actual"))  # confusion table

test$ct_bp_pred_prob<-predict(bp_tree,test)[,2]
test$ct_bp_pred_class=predict(bp_tree,test,type="class")
table(test$ct_bp_pred_class==test$income)  # error rate
table(test$ct_bp_pred_class,test$income, dnn=c("predicted","actual"))  # confusion table on test data
```
training accuracy = 86.9%
test accuracy = 85.62%

```{r}
ad_model<-rpart(income~age+workclass+fnlwgt+education+education.num+marital.status+occupation+relationship+race+sex+capital.gain+capital.loss+hours.per.week+native.country,           # model formula
                data=training,                     # dataset
                method="class",                   # "class" indicates a classification tree model 
                control=rpart.control(cp=0.004))   # tree control parameters. 
rpart.plot(ad_model)   # tree plot

training$ct_ad_pred_prob<-predict(ad_model,training)[,2]
training$ct_ad_pred_class=predict(ad_model,training,type="class")
table(training$ct_ad_pred_class==training$income)  # error rate
table(training$ct_ad_pred_class,training$income, dnn=c("predicted","actual"))  # confusion table

test$ct_ad_pred_prob<-predict(ad_model,test)[,2]
test$ct_ad_pred_class=predict(ad_model,test,type="class")
table(test$ct_ad_pred_class==test$income)  # error rate
table(test$ct_ad_pred_class,test$income, dnn=c("predicted","actual"))
```
training accuracy = 81.3%
test accuracy: 85%

Conclusion: We dicide to choose cp as 0.004. Under this value, the tree result is easily managebale and the tree performance seems to be great.

roc
```{r}
plot(mydata2, col = c("red", "blue"))
```

##### SVM

## set up initial model
```{r}

## significant attributes from glm ##income~age+workclass+education+marital.status+occupation+sex+relationship+capital.gain+capital.loss+hours.per.week

#training1<-training
#test1<-test


#training1$income[training1$income == '>50K'] <- 1
#training1$income[training1$income == '<=50K'] <- 0

model_svm<-svm(formula=income~age+workclass+fnlwgt+education+education.num+marital.status+occupation+relationship+race+sex+capital.gain+capital.loss+hours.per.week+native.country, 
               data=training,                   # dataset
               kernel="linear", # linear decision boundary
               cost=0.1,        # there are paremeters that are used to tune the model 
               scale=FALSE)

model_svm
## model takes really long to converge
```


```{r}
##View support vectors
head(model_svm$index)  #the support vectors of the model
```

```{r}
## Decision values are the distance between the observation and the decision boundary. The positive fitted value indicate one class, and negative value indicates the other class. 

dv<-data.frame(model_svm$decision.values) ## need dataframe to plot using ggplot
ggplot(dv, aes(x=model_svm$decision.values)) + 
  geom_histogram(binwidth=10000, colour="black",fill="white")
```

```{r}
head(model_svm$fitted)  # predicted (fitted) class
predicted_svm<-predict(model_svm,training,decision.values = TRUE)
head(attr(predicted_svm, "decision.values"))
```

```{r}

test$svm_pred_class <- predict(model_svm, test) 
table(test$svm_pred_class==test$income)  # error rate
table(test$svm_pred_class,test$income, dnn=c("predicted","actual"))
(3685+510)/6000
```
Accuracy = 69.91%

## Looking at target variable


## Tuning
We can tune SVM models using `tune` function. Set a range of search values for the parameter. It builds an SVM model for each possible combination of parameter values and evaluate accuracy. It will return the parameter combination that yields the best accuracy. 
```{r}
 # find a best set of parameters for the svm model
svm_tune <- tune(svm,                                      
                 income~age+workclass+fnlwgt+education+education.num+marital.status+occupation+relationship+race+sex+capital.gain+capital.loss+hours.per.week+native.country,
                 data = training,
                 kernel="linear", 
                 ranges = list(cost = 10^(-5:0))) # specifying the ranges of parameters  
                                                  # in the penalty function to be examined
                                                  # you may wish to increase the search space like 
                                                  

print(svm_tune)                              # best parameters for the model


```

```{r}
best_svm_mod <- svm_tune$best.model
test$svm_pred_class <- predict(best_svm_mod, test) # save the predicted class by the svm model
test$svm_dv<-as.numeric(attr(predict(best_svm_mod, test, decision.values = TRUE),"decision.values"))

table(test$income,test$svm_pred_class)
```
Accuracy = (4187+880)/6000 = 84.45%


#### Logistic Regression
```{r}
# build a logit regression model  
logit_model<-glm(income~age+workclass+fnlwgt+education+education.num+marital.status+occupation+relationship+race+sex+capital.gain+capital.loss+hours.per.week+native.country,  
                 family="binomial",               # specifying error distribution
                 data=mydata2)                    # dataset
summary(logit_model)

# Make predictions
mydata2$logit_pred_prob<-predict(logit_model,mydata2,type="response") # get predicted probabilities
mydata2$logit_pred_class<-ifelse(mydata2$logit_pred_prob>0.5,">50K","<=50K") 

## Confusion Matrix 
table(mydata2$income,mydata2$logit_pred_class)

```
accuracy = 84.98%

## Holdout validation
```{r}

# Train model on training dataset only 
logit_model_2<-glm(income~age+workclass+fnlwgt+education+education.num+marital.status+occupation+relationship+race+sex+capital.gain+capital.loss+hours.per.week+native.country,  
                 family="binomial",               # specifying error distribution
                 data=training)                    # dataset
summary(logit_model)

# Make predictions
training$logit_pred_prob<-predict(logit_model_2,training,type="response") # get predicted probabilities
training$logit_pred_class<-ifelse(training$logit_pred_prob>0.5,">50K","<=50K") 

## Confusion Matrix 
table(training$income,training$logit_pred_class)

```

```{r}
## Make prediction on test dataset
test$logit_pred_prob<-predict(logit_model_2,test,type="response")
test$logit_pred_class<-ifelse(test$logit_pred_prob>0.5,">50K","<=50K") 

## Confusion accuracy
table(test$income,test$logit_pred_class,dnn=c("Predicted","Actual"))


```
test accuracy=84.15%

##Random Forest



```{r}
#install.packages("randomForest")
library(randomForest)
rf_model<-randomForest(income~age+workclass+fnlwgt+education+education.num+marital.status+occupation+relationship+race+sex+capital.gain+capital.loss+hours.per.week+native.country,data=mydata2)
```



```{r}
#print(rf_model)
head(rf_model$votes)       # indicates the % of trees that voted for each class
head(rf_model$predicted)   # the class favored by more trees (i.e. majority vote wins) 
```

```{r}
varImpPlot(rf_model)  # importance of variables 

# Make prediction
mydata2$rf_model_prob<-predict(rf_model,mydata2,type="response")

## Confusion Matrix 
table(mydata2$income,mydata2$rf_model_prob,dnn=c("Predicted","Actual"))
```
accuracy=95.48%

```{r}
trainingRF<-training
testRF<-test
```

### hold-out validation vs. OOB errors
Following a similar process, we can validate the performance of a random forest model. 
```{r}
set.seed(1)
rf_model_oob<-randomForest(income~age+workclass+fnlwgt+education+education.num+marital.status+occupation+relationship+race+sex+capital.gain+capital.loss+hours.per.week+native.country,data=training,mtry=2,ntree=500, cutoff=c(0.5,0.5),importance=TRUE)
rf_model_oob

testRF
testRF$rf_cross_prob<-predict(rf_model_oob,testRF,type="prob")[,2]
testRF$rf_cross_class<-predict(rf_model_oob,testRF,type="class")
table(testRF$income==testRF$rf_cross_class) 
table(testRF$income,testRF$rf_cross_class,dnn=c("Predicted","Actual"))

```
OOB Error rate = 13.91%, OOB Accuracy=86.09%

Test Accuracy = (4227+978)/6000 = 86.63%

```{r}
# Execute the tuning process
#res <- tuneRF(x = trainingRF%>%select(age,workclass,fnlwgt,education,education.num,marital.status,occupation,relationship,race,sex,capital.gain,capital.loss,hours.per.week,native.country),y = trainingRF$income,mtryStart=2,ntreeTry = 500)
#res
```

### ROC Curves



```{r}
ct_roc<-roc(testRF$income,testRF$ct_ad_pred_prob,auc=TRUE)
plot(ct_roc,print.auc=TRUE,col="blue")

rf_roc<-roc(testRF$income,testRF$rf_cross_prob,auc=TRUE)
plot(rf_roc,print.auc=TRUE,print.auc.y=.4,col="green", add=TRUE)

logit_roc<-roc(testRF$income,testRF$logit_pred_prob,auc=TRUE)
plot(logit_roc,print.auc=TRUE,print.auc.y=.3, col="red",add=TRUE)

svm_roc<-roc(testRF$income,testRF$svm_dv,auc=TRUE)
plot(svm_roc,print.auc=TRUE,print.auc.y=.2, col="black",add=TRUE)
```



